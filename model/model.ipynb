{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7ca13a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Feature Setup ---\n",
      "Using features: ['windspeed', 'winddir', 'sealevelpressure', 'season', 'cloudcover']\n",
      "Categorical Features: ['winddir', 'season']\n",
      "Numerical Features: ['windspeed', 'sealevelpressure', 'cloudcover']\n",
      "Target columns: ['tempmin', 'tempmax', 'temp', 'feelslike']\n",
      "\n",
      "Data split: 1600 training samples, 400 testing samples\n",
      "\n",
      "Training the RandomForestRegressor model with reduced features...\n",
      "Model training complete.\n",
      "\n",
      "Making predictions on the test set...\n",
      "\n",
      "Evaluating Model Performance (Mean Absolute Error):\n",
      "  MAE for tempmin: 1.7692\n",
      "  MAE for tempmax: 2.1916\n",
      "  MAE for temp: 1.7784\n",
      "  MAE for feelslike: 1.8693\n",
      "\n",
      "Saving the trained model to random_forest_weather_predictor_reduced_features.joblib...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['random_forest_weather_predictor_reduced_features.joblib']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import OneHotEncoder,StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import numpy as np\n",
    "import joblib \n",
    "\n",
    "\n",
    "file_path = r'D:\\Weather api\\cairo_merged_with_season.csv' \n",
    "target_columns = ['tempmin', 'tempmax', 'temp', 'feelslike']\n",
    "\n",
    "final_feature_columns = ['windspeed', 'winddir', 'sealevelpressure', 'season', 'cloudcover']\n",
    "\n",
    "# Define the path and filename for saving the model\n",
    "model_save_path = 'random_forest_weather_predictor_reduced_features.joblib'\n",
    "\n",
    "# --- Load Data ---\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# --- Define Final Features (X) and Targets (y) ---\n",
    "X = df[final_feature_columns]\n",
    "y = df[target_columns]\n",
    "\n",
    "# --- Identify Column Types for Preprocessing ---\n",
    "categorical_features = ['winddir', 'season']\n",
    "numerical_features = [col for col in X.column   s if col not in categorical_features]\n",
    "\n",
    "# imputer: It fills in missing values in numerical features with the median value of the column of the season \n",
    "# scaler: standardizes the numerical features by removing the mean and scaling to unit variance\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Using most frequent for categorical imputation Converts categorical features into a one-hot encoded format, creating binary columns \n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "# Create the preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='passthrough' # Keep any columns not specified (shouldn't be any here)\n",
    ")\n",
    "\n",
    "\n",
    "# --- Create the Full Model Pipeline ---This combines the numerical and categorical transformers into a single ColumnTransformer:\n",
    "# n_estimators= 100 trees. \n",
    "# random_state=randomize the data for reproducibility\n",
    "# n_jobs=allows the model to use all available CPU cores for training\n",
    "model_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1))\n",
    "])\n",
    "\n",
    "#split_train \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# --- Train the Model ---\n",
    "model_pipeline.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model_pipeline.predict(X_test)\n",
    "# --- Evaluate the Model ---\n",
    "print(\"\\nEvaluating Model Performance (Mean Absolute Error):\")\n",
    "# Ensure y_pred has the expected shape before calculating MAE\n",
    "if y_pred.shape[1] == len(target_columns):\n",
    "    for i, target in enumerate(target_columns):\n",
    "        mae = mean_absolute_error(y_test[target], y_pred[:, i])\n",
    "        print(f\"  MAE for {target}: {mae:.4f}\")\n",
    "# --- Save the Trained Model ---\n",
    "print(f\"\\nSaving the trained model to {model_save_path}...\")\n",
    "joblib.dump(model_pipeline, model_save_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
